# -*- coding: utf-8 -*-
"""Online_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16eML-T9E1NCZYAK-uAqCeN-MXpedRsEE
"""

!pip install -q transformers accelerate bitsandbytes sentencepiece
!pip install -q torch torchvision torchaudio
!pip install -q gradio
!pip install -U bitsandbytes
!pip install -U transformers accelerate

from huggingface_hub import notebook_login
notebook_login()

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from transformers import MarianMTModel, MarianTokenizer
import gradio as gr



hi_en_model = "Helsinki-NLP/opus-mt-hi-en"
hi_en_tokenizer = MarianTokenizer.from_pretrained(hi_en_model)
hi_en_translator = MarianMTModel.from_pretrained(hi_en_model)

def translate_hi_to_en(text):
    inputs = hi_en_tokenizer(text, return_tensors="pt", padding=True)
    outputs = hi_en_translator.generate(**inputs)
    return hi_en_tokenizer.decode(outputs[0], skip_special_tokens=True)


en_hi_model = "Helsinki-NLP/opus-mt-en-hi"
en_hi_tokenizer = MarianTokenizer.from_pretrained(en_hi_model)
en_hi_translator = MarianMTModel.from_pretrained(en_hi_model)

def translate_en_to_hi(text):
    inputs = en_hi_tokenizer(text, return_tensors="pt", padding=True)
    outputs = en_hi_translator.generate(**inputs)
    return en_hi_tokenizer.decode(outputs[0], skip_special_tokens=True)



model_id = "mistralai/Mistral-7B-Instruct-v0.2"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16"
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=bnb_config
)

chatbot_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.7,
    top_p=0.9
)


def ask_bot(symptoms, lang="en"):
    # Step 1: If input is Hindi ‚Üí translate to English
    if lang == "hi":
        symptoms = translate_hi_to_en(symptoms)

    # Step 2: Create English medical prompt
    prompt = f"""
    You are a helpful medical assistant.
    The user will give symptoms.
    Respond with:
    - A possible condition (not a diagnosis, just an idea).
    - A simple home remedy or self-care advice.
    - A disclaimer that this is not medical advice.

    User symptoms: {symptoms}
    """


    response = chatbot_pipeline(prompt)[0]["generated_text"]


    response = response.replace(prompt, "").strip()


    if lang == "hi":
        response = translate_en_to_hi(response)

    return response

# ========================================
# 6. Gradio UI
# ========================================
with gr.Blocks() as demo:
    gr.Markdown("## üåç Multilingual Symptom Checker Chatbot\nSupports **English + Hindi**")

    chatbot_ui = gr.Chatbot(height=400)
    msg = gr.Textbox(placeholder="Type your symptoms here in Hindi or English...")
    clear = gr.Button("Clear Chat")

    def respond(message, chat_history):
        # Detect Hindi (simple check: Devanagari characters)
        lang = "hi" if any("\u0900" <= c <= "\u097F" for c in message) else "en"
        bot_reply = ask_bot(message, lang)
        chat_history.append((message, bot_reply))
        return "", chat_history

    msg.submit(respond, [msg, chatbot_ui], [msg, chatbot_ui])
    clear.click(lambda: None, None, chatbot_ui, queue=False)

demo.launch(share=True)